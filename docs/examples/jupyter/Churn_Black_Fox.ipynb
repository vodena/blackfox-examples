{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Churn\n  \n### Problem explanation:\n\nThe customer churn, also known as customer attrition, refers to the phenomenon whereby a customer leaves a company. Some studies confirmed that acquiring new customers can cost five times more than satisfying and retaining existing customers. As a matter of fact, there are a lot of benefits that encourage the tracking of the customer churn rate, for example, marketing costs to acquire new customers are high. Therefore, it is important to retain customers so that the initial investment is not wasted, It has a direct impact on the ability to expand the company, etc.\n\nA bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records data set to investigate and predict which of the customers are more likely to leave the bank soon. The data set itself is located here, in the field Artificial_Neural_Networks.\n\nThis is classification problem and the results are two outputs, customer will leave the bank or he will not. Model inputs are:\n \n* Credit score,\n* Geography,\n* Gender,\n* Age,\n* Tenure,\n* Balance,\n* Number of products,\n* Has credit card,\n* Is active member,\n* Estimated salary.\n\n### Problem solution:\nData set contains 10000 observations,we have divided the data set in two sets, training set, which contains 8000 observations and test set, which contains 2000 observations. We solved problem in two ways, with Python and Black Fox. Model performance was measured with K-cross validation (K=5) and for feature scaling we used min-max scaler. Inputs geography and gender are categorical data, so they were encoded with one hot encoder (to avoid dummy variable trap, for geography we ignored, for example Germany and for gender we ignored female). To stop training at the right time we used Early Stopping."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Update Keras to latest version:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install keras==2.2.4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Data preprocessing\n#### Importing data frame:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Importing the date as data frame wich we will import with pandas using the read_csv function.\ndataframe = pd.read_csv('Churn_Modelling.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Dataset info:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataframe.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Dataset description:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataframe.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Dataset histogram:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataframe.hist(figsize=(10,10));",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Corelation heatmap:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.heatmap(dataframe.corr(), vmin=0, vmax=1);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "####  We will separate data frame into matrix X of features(where we will trow first 3 columns because they are useless) and dependent variable which is matrix y.  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = dataframe.iloc[:, 3:13].values\ny = dataframe.iloc[:, 13:14].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### The next code section is about splitting the data set into the training set and test set but before we do that we must pay attention on categorical variables in our matrix of features and there for we need to encode them."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\n\n# Encoding geography( countries )\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\n\n# Encoding gender\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n\n# OneHotEncoding the countries to make dummy variables for this categorical variable.\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\n\n# To avoid dummy variable trap we remove for example countre Germany.\nX = X[:, 1:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Now we are able to split the dataset into the training set and test set.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Black fox service finding best ANN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Install Black fox service:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install blackfox-1.0.0.tar.gz",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Let's run Black Fox service to find best ANN:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the BF service libraries and other libraries\nfrom blackfox import BlackFox\nfrom blackfox import KerasOptimizationConfig\nfrom blackfox import OptimizationEngineConfig\nimport h5py\n#from keras.models import load_model\n#import numpy as np\n#import pandas as pd\n\nblackfox_url = 'http://147.91.204.14:32701'\nbf = BlackFox(blackfox_url)\n\nec = OptimizationEngineConfig(proc_timeout_miliseconds=2000000, population_size=50, max_num_of_generations=20)\nc = KerasOptimizationConfig(engine_config=ec, max_epoch = 3000, validation_split=0.3)\n\n# Use CTRL + C to stop optimization\n(ann_io, ann_info, ann_metadata) = bf.optimize_keras_sync(\n    input_set = X_train,\n    output_set = y_train,\n    config = c,\n    integrate_scaler=False,\n    network_path='OptimizedChurnWithBF.h5'\n)\n\nprint('\\nann info:')\nprint(ann_info)\n\nprint('\\nann metadata:')\nprint(ann_metadata)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Data that we transfer to Black Fox service are not scaled, the service will scale the date by its own and when he finish his job he won't change the data, but service ofers us command to scale our data for prediction as he did and we will ofcourse use that."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get metadata\nmeta = bf.get_metadata('OptimizedChurnWithBF.h5')\nscaler_config = meta['scaler_config']\n\n# Scale\nx_scaler_config = scaler_config['input']\nfrom sklearn.preprocessing import MinMaxScaler \nmin_max_x = MinMaxScaler(feature_range=x_scaler_config['feature_range'])\nmin_max_x.fit(x_scaler_config['fit'])\n\nX_test_minMaxScaled_withBF = min_max_x.transform(X_test)\n#print(X_test_minMaxScaled_withBF[:10,:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Prediction:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing ANN model\nfrom keras.models import load_model\nmodel = load_model('OptimizedChurnWithBF.h5')\n\n# Predicted values\ny_pred_BF=model.predict(X_test_minMaxScaled_withBF)\n#print(\"Predicted values are:\\n\\n\", y_pred_BF[:20,:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Restoring the results on real values:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Rescale\ny_scaler_config = scaler_config['output']\nmin_max_y = MinMaxScaler(feature_range=y_scaler_config['feature_range'])\nmin_max_y.fit(y_scaler_config['fit'])\n\ny_pred_BF_realValues = min_max_y.inverse_transform(y_pred_BF)\n#print(\"\\nFirst 6 real predicted values are:\\n\", y_pred_BF_realValues[:6,:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Calculating the error:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred_BF_realValues_rounded = (y_pred_BF_realValues[:,0] > y_pred_BF_realValues[:,1])\ny_pred_BF_realValues_rounded = np.where(y_pred_BF_realValues_rounded == True, 1, 0)\ny_test_for_confusionMatrix = (y_test[:,0] > y_test[:,1])\ny_test_for_confusionMatrix = np.where(y_test_for_confusionMatrix == True, 1, 0)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test_for_confusionMatrix, y_pred_BF_realValues_rounded)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_winner)\n\nerrorOnTestSetBF = 100*(cm[0,1]+cm[1,0])/y_test.shape[0]\n\nprint(\"\\nWe got confusion matrix:\\n\",cm)\nprint(\"\\nTest set error for finding the best ANN by Black Fox service, which we can read in confusion matrix is\",errorOnTestSetBF,\"%.\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}